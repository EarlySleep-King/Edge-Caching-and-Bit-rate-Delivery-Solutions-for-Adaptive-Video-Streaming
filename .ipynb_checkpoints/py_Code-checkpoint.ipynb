{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a962bca-8994-4151-b1b4-cd702e6e8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a7cc0-d107-49d7-a59b-d365a6d73f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('GBvideos_with_quality.csv')\n",
    "\n",
    "# Convert date format\n",
    "data['publish_time'] = pd.to_datetime(data['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna(subset=['views', 'likes', 'publish_time', 'tags', 'category_id', 'quality', 'adjusted_views', 'comment_count'])\n",
    "\n",
    "# Preserve the original quality field before encoding\n",
    "data['quality_original'] = data['quality'].copy()\n",
    "\n",
    "# Label encoding for tags\n",
    "label_encoder = LabelEncoder()\n",
    "data['tags'] = label_encoder.fit_transform(data['tags'])\n",
    "\n",
    "# Label encoding for quality\n",
    "quality_encoder = LabelEncoder()\n",
    "data['quality'] = quality_encoder.fit_transform(data['quality'])\n",
    "\n",
    "# Time of day for publishing time\n",
    "data['time_of_day'] = data['publish_time'].dt.hour // 6 \n",
    "category_ids = data['category_id'].unique()\n",
    "\n",
    "# Define custom Prophet model class\n",
    "class CustomProphet(Prophet):\n",
    "    def __init__(self, custom_trend_param=0.5, custom_seasonality_param=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.custom_trend_param = custom_trend_param\n",
    "        self.custom_seasonality_param = custom_seasonality_param\n",
    "        self.custom_regressors = []\n",
    "\n",
    "    def add_custom_regressor(self, name, **kwargs):\n",
    "        self.custom_regressors.append(name)\n",
    "        self.add_regressor(name, **kwargs)\n",
    "\n",
    "    def fit(self, df, **kwargs):\n",
    "        self.add_seasonality(name='quarterly', period=90.25, fourier_order=8)\n",
    "        super().fit(df, **kwargs)\n",
    "        self.params['custom_trend'] = self._fit_custom_trend(df)\n",
    "        self.params['custom_seasonality'] = self._fit_custom_seasonality(df)\n",
    "        self.params['custom_regressors'] = self._fit_custom_regressors(df)\n",
    "        return self\n",
    "\n",
    "    def _fit_custom_trend(self, df):\n",
    "        dates = pd.to_datetime(df['ds']).map(pd.Timestamp.toordinal).values\n",
    "        y = df['y'].values\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(dates.reshape(-1, 1), y)\n",
    "        self.custom_trend_model = model\n",
    "        return model\n",
    "\n",
    "    def _predict_custom_trend(self, df):\n",
    "        if hasattr(self, 'custom_trend_model'):\n",
    "            dates = pd.to_datetime(df['ds']).map(pd.Timestamp.toordinal).values\n",
    "            trend = self.custom_trend_model.predict(dates.reshape(-1, 1))\n",
    "            return trend\n",
    "        else:\n",
    "            raise ValueError(\"Custom trend model has not been fitted.\")\n",
    "\n",
    "    def _fit_custom_seasonality(self, df):\n",
    "        dates = pd.to_datetime(df['ds']).map(pd.Timestamp.toordinal).values\n",
    "        y = df['y'].values\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree=4)\n",
    "        dates_poly = poly.fit_transform(dates.reshape(-1, 1))\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(dates_poly, y)\n",
    "        self.custom_seasonality_model = model\n",
    "        self.poly = poly\n",
    "        return model\n",
    "\n",
    "    def _predict_custom_seasonality(self, df):\n",
    "        if hasattr(self, 'custom_seasonality_model'):\n",
    "            dates = pd.to_datetime(df['ds']).map(pd.Timestamp.toordinal).values\n",
    "            dates_poly = self.poly.transform(dates.reshape(-1, 1))\n",
    "            seasonality = self.custom_seasonality_model.predict(dates_poly)\n",
    "            return seasonality\n",
    "        else:\n",
    "            raise ValueError(\"Custom seasonality model has not been fitted.\")\n",
    "\n",
    "    def _fit_custom_regressors(self, df):\n",
    "        regressors = df[self.custom_regressors]\n",
    "        y = df['y'].values\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(regressors, y)\n",
    "        self.custom_regressors_model = model\n",
    "        return model\n",
    "\n",
    "    def _predict_custom_regressors(self, df):\n",
    "        if hasattr(self, 'custom_regressors_model'):\n",
    "            regressors = df[self.custom_regressors]\n",
    "            regressors_pred = self.custom_regressors_model.predict(regressors)\n",
    "            return regressors_pred\n",
    "        else:\n",
    "            raise ValueError(\"Custom regressors model has not been fitted.\")\n",
    "\n",
    "    def predict(self, df):\n",
    "        forecast = super().predict(df)\n",
    "        forecast['custom_trend'] = self._predict_custom_trend(df)\n",
    "        forecast['custom_seasonality'] = self._predict_custom_seasonality(df)\n",
    "        forecast['custom_regressors'] = self._predict_custom_regressors(df)\n",
    "        forecast['yhat'] += (forecast['custom_trend'] +\n",
    "                             forecast['custom_seasonality'] +\n",
    "                             forecast['custom_regressors'])\n",
    "        forecast['yhat'] = forecast['yhat'].apply(lambda x: max(0, x))\n",
    "        return forecast\n",
    "\n",
    "def get_predictions(category_data):\n",
    "    df = pd.DataFrame()\n",
    "    df['ds'] = category_data['publish_time']\n",
    "    df['y'] = category_data['adjusted_views']\n",
    "    df['tags'] = category_data['tags']\n",
    "    df['likes'] = category_data['likes']\n",
    "    df['quality'] = category_data['quality']\n",
    "    df['time_of_day'] = category_data['time_of_day']\n",
    "\n",
    "    df['like_rate'] = category_data['likes'] / category_data['views']\n",
    "    df['days_since_publish'] = (df['ds'] - df['ds'].min()).dt.days\n",
    "    df['is_weekend'] = df['ds'].dt.weekday.isin([5, 6]).astype(int)\n",
    "    df['comment_count'] = category_data['comment_count']\n",
    "\n",
    "    model = CustomProphet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False, seasonality_mode='multiplicative')\n",
    "    model.add_custom_regressor('tags')\n",
    "    model.add_custom_regressor('likes')\n",
    "    model.add_custom_regressor('quality')\n",
    "    model.add_custom_regressor('days_since_publish')\n",
    "    model.add_custom_regressor('is_weekend')\n",
    "    model.add_custom_regressor('time_of_day')\n",
    "    model.add_custom_regressor('like_rate')\n",
    "    model.add_custom_regressor('comment_count')\n",
    "\n",
    "    model.fit(df)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=365)\n",
    "    future['tags'] = 0\n",
    "    future['likes'] = 0\n",
    "    future['quality'] = 0\n",
    "    future['time_of_day'] = 0\n",
    "    future['days_since_publish'] = (future['ds'] - df['ds'].min()).dt.days\n",
    "    future['is_weekend'] = future['ds'].dt.weekday.isin([5, 6]).astype(int)\n",
    "    future['like_rate'] = 0\n",
    "    future['comment_count'] = 0\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "    forecast['category_id'] = category_data['category_id'].iloc[0]\n",
    "    forecast['quality'] = category_data['quality'].iloc[0]\n",
    "    return forecast\n",
    "\n",
    "# Function to run multiple simulations and average the results\n",
    "def run_simulations(alg_func, df, cache_size, num_simulations=5):\n",
    "    total_hit_ratio = 0\n",
    "    for _ in range(num_simulations):\n",
    "        total_hit_ratio += alg_func(df, cache_size)\n",
    "    avg_hit_ratio = total_hit_ratio / num_simulations\n",
    "    return avg_hit_ratio\n",
    "\n",
    "# Collect all forecasts\n",
    "all_forecasts = []\n",
    "for category_id in category_ids:\n",
    "    category_data = data[data['category_id'] == category_id]\n",
    "    forecast = get_predictions(category_data)\n",
    "    all_forecasts.append(forecast)\n",
    "\n",
    "# Combine all forecasts\n",
    "all_forecasts_df = pd.concat(all_forecasts).reset_index(drop=True)\n",
    "all_forecasts_df['adjusted_views'] = all_forecasts_df['yhat']  # Assuming adjusted_views is based on yhat\n",
    "\n",
    "# Define function to calculate hit ratio\n",
    "def calculate_custom_hit_ratio(df, cache_size, total_requests=1000):\n",
    "    df_sorted = df.sort_values(by='yhat', ascending=False)\n",
    "    cached_videos = df_sorted.head(cache_size)\n",
    "    cached_video_ids = set(cached_videos.index)\n",
    "\n",
    "    hits = 0\n",
    "    for i in range(total_requests):\n",
    "        probabilities = (df['yhat'] * 2 + df['likes'] * 1.5 + df['comment_count'] * 1.2 + df['adjusted_views']).fillna(0) + 1e-9\n",
    "        probabilities = probabilities.clip(lower=0)\n",
    "        if probabilities.sum() > 0:\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        else:\n",
    "            probabilities = np.ones(len(probabilities)) / len(probabilities)\n",
    "        requested_video = np.random.choice(df.index, p=probabilities)\n",
    "        if requested_video in cached_video_ids:\n",
    "            hits += 1\n",
    "\n",
    "    cache_hit_ratio = hits / total_requests\n",
    "    return cache_hit_ratio\n",
    "\n",
    "# LRU\n",
    "def calculate_LRU_hit_ratio(df, cache_size, total_requests=1000):\n",
    "    df_sorted = df.sort_values(by='ds', ascending=False) \n",
    "    cached_videos = df_sorted.head(cache_size)\n",
    "    cached_video_ids = set(cached_videos.index)\n",
    "\n",
    "    hits = 0\n",
    "    for i in range(total_requests):\n",
    "        probabilities = (df['yhat'] * 2 + df['likes'] * 1.5 + df['comment_count'] * 1.2 + df['adjusted_views']).fillna(0) + 1e-9\n",
    "\n",
    "        probabilities = probabilities.clip(lower=0)\n",
    "        if probabilities.sum() > 0:\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        else:\n",
    "            probabilities = np.ones(len(probabilities)) / len(probabilities)\n",
    "\n",
    "        requested_video = np.random.choice(df.index, p=probabilities)\n",
    "        if requested_video in cached_video_ids:\n",
    "            hits += 1\n",
    "\n",
    "    cache_hit_ratio = hits / total_requests\n",
    "    return cache_hit_ratio\n",
    "\n",
    "# FIFO\n",
    "def calculate_FIFO_hit_ratio(df, cache_size, total_requests=1000):\n",
    "    df_sorted = df.sort_values(by='ds', ascending=True) \n",
    "    cached_videos = df_sorted.head(cache_size)\n",
    "    cached_video_ids = set(cached_videos.index)\n",
    "\n",
    "    hits = 0\n",
    "    for i in range(total_requests):\n",
    "        probabilities = (df['yhat'] * 2 + df['likes'] * 1.5 + df['comment_count'] * 1.2 + df['adjusted_views']).fillna(0) + 1e-9\n",
    "\n",
    "        probabilities = probabilities.clip(lower=0)\n",
    "        if probabilities.sum() > 0:\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "        else:\n",
    "            probabilities = np.ones(len(probabilities)) / len(probabilities)\n",
    "\n",
    "        requested_video = np.random.choice(df.index, p=probabilities)\n",
    "        if requested_video in cached_video_ids:\n",
    "            hits += 1\n",
    "\n",
    "    cache_hit_ratio = hits / total_requests\n",
    "    return cache_hit_ratio\n",
    "\n",
    "# Define dictionary for algorithms\n",
    "algorithms = {\n",
    "    'Custom': calculate_custom_hit_ratio,\n",
    "    'LRU': calculate_LRU_hit_ratio,\n",
    "    'FIFO': calculate_FIFO_hit_ratio \n",
    "}\n",
    "\n",
    "# Initialize result storage\n",
    "results = {\n",
    "    'video_counts': [],\n",
    "    'user_counts': [],\n",
    "    'server_counts': [],\n",
    "    'server_capacities': [],\n",
    "    'quality_levels': []\n",
    "}\n",
    "\n",
    "# Use algorithms\n",
    "video_counts = [100, 500, 1000, 1500, 2000]\n",
    "for count in video_counts:\n",
    "    subset = all_forecasts_df.sample(n=count)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        cache_hit_ratio = run_simulations(alg_func, subset, cache_size=500)\n",
    "        results['video_counts'].append((count, 'All Qualities', alg_name, cache_hit_ratio))\n",
    "\n",
    "# Simulate different user counts\n",
    "user_counts = [1000, 5000, 10000, 20000, 50000]\n",
    "for users in user_counts:\n",
    "    subset = all_forecasts_df.sample(n=1000)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        cache_hit_ratio = run_simulations(alg_func, subset, cache_size=500)\n",
    "        results['user_counts'].append((users, 'All Qualities', alg_name, cache_hit_ratio))\n",
    "\n",
    "# Simulate different server counts\n",
    "server_counts = [1, 5, 10, 20, 50]\n",
    "for servers in server_counts:\n",
    "    subset = all_forecasts_df.sample(n=1000)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        cache_hit_ratio = run_simulations(alg_func, subset, cache_size=500)\n",
    "        results['server_counts'].append((servers, 'All Qualities', alg_name, cache_hit_ratio))\n",
    "\n",
    "# Simulate different server capacities\n",
    "server_capacities = [100, 300, 500, 700, 1000]\n",
    "for capacity in server_capacities:\n",
    "    subset = all_forecasts_df\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        cache_hit_ratio = run_simulations(alg_func, subset, cache_size=capacity)\n",
    "        results['server_capacities'].append((capacity, 'All Qualities', alg_name, cache_hit_ratio))\n",
    "\n",
    "# Simulate different video qualities\n",
    "quality_levels = data['quality_original'].unique()\n",
    "for quality in quality_levels:\n",
    "    subset = all_forecasts_df[all_forecasts_df['quality'] == quality_encoder.transform([quality])[0]]\n",
    "    if not subset.empty:\n",
    "        for alg_name, alg_func in algorithms.items():\n",
    "            cache_hit_ratio = run_simulations(alg_func, subset, cache_size=500)\n",
    "            results['quality_levels'].append((quality, alg_name, cache_hit_ratio))\n",
    "    else:\n",
    "        print(f\"No data available for quality: {quality}, skipping.\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Comparison of Caching Algorithms')\n",
    "\n",
    "# Plot by video count\n",
    "video_counts_df = pd.DataFrame(results['video_counts'], columns=['Video Count', 'Quality', 'Algorithm', 'Hit Ratio'])\n",
    "for alg in video_counts_df['Algorithm'].unique():\n",
    "    subset = video_counts_df[video_counts_df['Algorithm'] == alg]\n",
    "    axes[0, 0].plot(subset['Video Count'], subset['Hit Ratio'], label=alg)\n",
    "axes[0, 0].set_title('Hit Ratio by Video Count')\n",
    "axes[0, 0].set_xlabel('Video Count')\n",
    "axes[0, 0].set_ylabel('Hit Ratio')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot by user count\n",
    "user_counts_df = pd.DataFrame(results['user_counts'], columns=['User Count', 'Quality', 'Algorithm', 'Hit Ratio'])\n",
    "for alg in user_counts_df['Algorithm'].unique():\n",
    "    subset = user_counts_df[user_counts_df['Algorithm'] == alg]\n",
    "    axes[0, 1].plot(subset['User Count'], subset['Hit Ratio'], label=alg)\n",
    "axes[0, 1].set_title('Hit Ratio by User Count')\n",
    "axes[0, 1].set_xlabel('User Count')\n",
    "axes[0, 1].set_ylabel('Hit Ratio')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot by server count\n",
    "server_counts_df = pd.DataFrame(results['server_counts'], columns=['Server Count', 'Quality', 'Algorithm', 'Hit Ratio'])\n",
    "for alg in server_counts_df['Algorithm'].unique():\n",
    "    subset = server_counts_df[server_counts_df['Algorithm'] == alg]\n",
    "    axes[1, 0].plot(subset['Server Count'], subset['Hit Ratio'], label=alg)\n",
    "axes[1, 0].set_title('Hit Ratio by Server Count')\n",
    "axes[1, 0].set_xlabel('Server Count')\n",
    "axes[1, 0].set_ylabel('Hit Ratio')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot by server capacity\n",
    "server_capacities_df = pd.DataFrame(results['server_capacities'], columns=['Server Capacity', 'Quality', 'Algorithm', 'Hit Ratio'])\n",
    "for alg in server_capacities_df['Algorithm'].unique():\n",
    "    subset = server_capacities_df[server_capacities_df['Algorithm'] == alg]\n",
    "    axes[1, 1].plot(subset['Server Capacity'], subset['Hit Ratio'], label=alg)\n",
    "axes[1, 1].set_title('Hit Ratio by Server Capacity')\n",
    "axes[1, 1].set_xlabel('Server Capacity')\n",
    "axes[1, 1].set_ylabel('Hit Ratio')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot by video quality as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "quality_levels_df = pd.DataFrame(results['quality_levels'], columns=['Quality', 'Algorithm', 'Hit Ratio'])\n",
    "if not quality_levels_df.empty:\n",
    "    quality_levels_df = quality_levels_df.pivot(index='Algorithm', columns='Quality', values='Hit Ratio')\n",
    "    quality_levels_df.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Hit Ratio by Video Quality')\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel('Hit Ratio')\n",
    "    plt.legend(title='Video Quality')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting the bar chart.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
